{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f508c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import streamlit as st\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6565e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb7a48c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 16:21:24.889 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "@st.cache_data # a way to reloadthe streamlit once\n",
    "def load_data():\n",
    "    \n",
    "    df_agg = pd.read_csv('Aggregated_Metrics_By_Video.csv').iloc[1:,:] #skipp the first row\n",
    "    df_agg.columns = ['Video','Video title','Video publish time','Comments added','Shares','Dislikes','Likes',\n",
    "                      'Subscribers lost','Subscribers gained','RPM(USD)','CPM(USD)','Average % viewed','Average view duration',\n",
    "                      'Views','Watch time (hours)','Subscribers','Your estimated revenue (USD)','Impressions','Impressions ctr(%)']\n",
    "    df_agg['Video publish time'] = pd.to_datetime(df_agg['Video publish time'])\n",
    "    df_agg['Average view duration'] = df_agg['Average view duration'].apply(lambda x: datetime.strptime(x,'%H:%M:%S'))\n",
    "    df_agg['Avg_duration_sec'] = df_agg['Average view duration'].apply(lambda x: x.second + x.minute*60 + x.hour*3600)\n",
    "    df_agg['Engagement_ratio'] =  (df_agg['Comments added'] + df_agg['Shares'] +df_agg['Dislikes'] + df_agg['Likes']) /df_agg.Views\n",
    "    df_agg['Views / sub gained'] = df_agg['Views'] / df_agg['Subscribers gained']\n",
    "    df_agg.sort_values('Video publish time', ascending = False, inplace = True)    \n",
    "    df_agg_sub = pd.read_csv('Aggregated_Metrics_By_Country_And_Subscriber_Status.csv')\n",
    "    df_comments = pd.read_csv('Aggregated_Metrics_By_Video.csv')\n",
    "    df_time = pd.read_csv('Video_Performance_Over_Time.csv')\n",
    "    df_time['Date'] = pd.to_datetime(df_time['Date'])\n",
    "    return df_agg, df_agg_sub, df_comments, df_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d6bdfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 16:21:30.285 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\kelida\\Desktop\\streamlit\\stenv\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2023-03-28 16:21:30.292 No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "df_agg, df_agg_sub, df_comments, df_time = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0762c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buidl a dashboard\n",
    "add_sidebar = st.sidebar.selectbox('Aggreggate or Individual Video', ('Aggregate Metrics', 'Individual Video Analysis'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63111a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_diff = df_agg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34b33f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video</th>\n",
       "      <th>Video title</th>\n",
       "      <th>Video publish time</th>\n",
       "      <th>Comments added</th>\n",
       "      <th>Shares</th>\n",
       "      <th>Dislikes</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Subscribers lost</th>\n",
       "      <th>Subscribers gained</th>\n",
       "      <th>RPM(USD)</th>\n",
       "      <th>...</th>\n",
       "      <th>Average view duration</th>\n",
       "      <th>Views</th>\n",
       "      <th>Watch time (hours)</th>\n",
       "      <th>Subscribers</th>\n",
       "      <th>Your estimated revenue (USD)</th>\n",
       "      <th>Impressions</th>\n",
       "      <th>Impressions ctr(%)</th>\n",
       "      <th>Avg_duration_sec</th>\n",
       "      <th>Engagement_ratio</th>\n",
       "      <th>Views / sub gained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0jTtHYie3CU</td>\n",
       "      <td>Should You Be Excited About Web 3? (As a Data ...</td>\n",
       "      <td>2022-01-17</td>\n",
       "      <td>37</td>\n",
       "      <td>43</td>\n",
       "      <td>8</td>\n",
       "      <td>267</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>4.055</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01 00:02:38</td>\n",
       "      <td>4383</td>\n",
       "      <td>192.5779</td>\n",
       "      <td>4</td>\n",
       "      <td>16.549</td>\n",
       "      <td>65130</td>\n",
       "      <td>2.95</td>\n",
       "      <td>158</td>\n",
       "      <td>0.080995</td>\n",
       "      <td>243.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2RWwN5ZT4tA</td>\n",
       "      <td>Should  @Luke Barousse Take This Data Analyst ...</td>\n",
       "      <td>2022-01-14</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.882</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01 00:00:38</td>\n",
       "      <td>2401</td>\n",
       "      <td>25.9375</td>\n",
       "      <td>0</td>\n",
       "      <td>1.720</td>\n",
       "      <td>25094</td>\n",
       "      <td>2.64</td>\n",
       "      <td>38</td>\n",
       "      <td>0.039567</td>\n",
       "      <td>2401.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>rEWPqw6rMGI</td>\n",
       "      <td>The Only Data Science Explanation You Need</td>\n",
       "      <td>2022-01-10</td>\n",
       "      <td>62</td>\n",
       "      <td>141</td>\n",
       "      <td>5</td>\n",
       "      <td>722</td>\n",
       "      <td>28</td>\n",
       "      <td>136</td>\n",
       "      <td>5.971</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01 00:04:40</td>\n",
       "      <td>10277</td>\n",
       "      <td>801.5549</td>\n",
       "      <td>108</td>\n",
       "      <td>60.498</td>\n",
       "      <td>215491</td>\n",
       "      <td>2.22</td>\n",
       "      <td>280</td>\n",
       "      <td>0.090493</td>\n",
       "      <td>75.566176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>o-wsyxWbPOw</td>\n",
       "      <td>We Need to Talk About The LinkedIn Machine Lea...</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>65</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>592</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>5.321</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01 00:02:46</td>\n",
       "      <td>11808</td>\n",
       "      <td>545.6332</td>\n",
       "      <td>68</td>\n",
       "      <td>62.568</td>\n",
       "      <td>166915</td>\n",
       "      <td>3.32</td>\n",
       "      <td>166</td>\n",
       "      <td>0.059705</td>\n",
       "      <td>151.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>xpIFS6jZbe8</td>\n",
       "      <td>How I Would Learn Data Science in 2022 (If I H...</td>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>109</td>\n",
       "      <td>767</td>\n",
       "      <td>53</td>\n",
       "      <td>4413</td>\n",
       "      <td>46</td>\n",
       "      <td>2553</td>\n",
       "      <td>6.836</td>\n",
       "      <td>...</td>\n",
       "      <td>1900-01-01 00:04:29</td>\n",
       "      <td>79283</td>\n",
       "      <td>5945.5420</td>\n",
       "      <td>2507</td>\n",
       "      <td>528.286</td>\n",
       "      <td>1420968</td>\n",
       "      <td>3.31</td>\n",
       "      <td>269</td>\n",
       "      <td>0.067379</td>\n",
       "      <td>31.054837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Video                                        Video title  \\\n",
       "111  0jTtHYie3CU  Should You Be Excited About Web 3? (As a Data ...   \n",
       "187  2RWwN5ZT4tA  Should  @Luke Barousse Take This Data Analyst ...   \n",
       "64   rEWPqw6rMGI         The Only Data Science Explanation You Need   \n",
       "59   o-wsyxWbPOw  We Need to Talk About The LinkedIn Machine Lea...   \n",
       "32   xpIFS6jZbe8  How I Would Learn Data Science in 2022 (If I H...   \n",
       "\n",
       "    Video publish time  Comments added  Shares  Dislikes  Likes  \\\n",
       "111         2022-01-17              37      43         8    267   \n",
       "187         2022-01-14              12       2         3     78   \n",
       "64          2022-01-10              62     141         5    722   \n",
       "59          2022-01-03              65      36        12    592   \n",
       "32          2021-12-27             109     767        53   4413   \n",
       "\n",
       "     Subscribers lost  Subscribers gained  RPM(USD)  ...  \\\n",
       "111                14                  18     4.055  ...   \n",
       "187                 1                   1     1.882  ...   \n",
       "64                 28                 136     5.971  ...   \n",
       "59                 10                  78     5.321  ...   \n",
       "32                 46                2553     6.836  ...   \n",
       "\n",
       "     Average view duration  Views Watch time (hours)  Subscribers  \\\n",
       "111    1900-01-01 00:02:38   4383           192.5779            4   \n",
       "187    1900-01-01 00:00:38   2401            25.9375            0   \n",
       "64     1900-01-01 00:04:40  10277           801.5549          108   \n",
       "59     1900-01-01 00:02:46  11808           545.6332           68   \n",
       "32     1900-01-01 00:04:29  79283          5945.5420         2507   \n",
       "\n",
       "     Your estimated revenue (USD)  Impressions  Impressions ctr(%)  \\\n",
       "111                        16.549        65130                2.95   \n",
       "187                         1.720        25094                2.64   \n",
       "64                         60.498       215491                2.22   \n",
       "59                         62.568       166915                3.32   \n",
       "32                        528.286      1420968                3.31   \n",
       "\n",
       "     Avg_duration_sec  Engagement_ratio  Views / sub gained  \n",
       "111               158          0.080995          243.500000  \n",
       "187                38          0.039567         2401.000000  \n",
       "64                280          0.090493           75.566176  \n",
       "59                166          0.059705          151.384615  \n",
       "32                269          0.067379           31.054837  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f25c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Video', 'Video title', 'Video publish time', 'Comments added',\n",
       "       'Shares', 'Dislikes', 'Likes', 'Subscribers lost', 'Subscribers gained',\n",
       "       'RPM(USD)', 'CPM(USD)', 'Average % viewed', 'Average view duration',\n",
       "       'Views', 'Watch time (hours)', 'Subscribers',\n",
       "       'Your estimated revenue (USD)', 'Impressions', 'Impressions ctr(%)',\n",
       "       'Avg_duration_sec', 'Engagement_ratio', 'Views / sub gained'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_agg_diff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b65f8665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 223 entries, 111 to 223\n",
      "Data columns (total 22 columns):\n",
      " #   Column                        Non-Null Count  Dtype         \n",
      "---  ------                        --------------  -----         \n",
      " 0   Video                         223 non-null    object        \n",
      " 1   Video title                   223 non-null    object        \n",
      " 2   Video publish time            223 non-null    datetime64[ns]\n",
      " 3   Comments added                223 non-null    int64         \n",
      " 4   Shares                        223 non-null    int64         \n",
      " 5   Dislikes                      223 non-null    int64         \n",
      " 6   Likes                         223 non-null    int64         \n",
      " 7   Subscribers lost              223 non-null    int64         \n",
      " 8   Subscribers gained            223 non-null    int64         \n",
      " 9   RPM(USD)                      223 non-null    float64       \n",
      " 10  CPM(USD)                      221 non-null    float64       \n",
      " 11  Average % viewed              223 non-null    float64       \n",
      " 12  Average view duration         223 non-null    datetime64[ns]\n",
      " 13  Views                         223 non-null    int64         \n",
      " 14  Watch time (hours)            223 non-null    float64       \n",
      " 15  Subscribers                   223 non-null    int64         \n",
      " 16  Your estimated revenue (USD)  223 non-null    float64       \n",
      " 17  Impressions                   223 non-null    int64         \n",
      " 18  Impressions ctr(%)            223 non-null    float64       \n",
      " 19  Avg_duration_sec              223 non-null    int64         \n",
      " 20  Engagement_ratio              223 non-null    float64       \n",
      " 21  Views / sub gained            223 non-null    float64       \n",
      "dtypes: datetime64[ns](2), float64(8), int64(10), object(2)\n",
      "memory usage: 40.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_agg_diff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ead6f18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments added                      43.500000\n",
      "Shares                              42.500000\n",
      "Dislikes                             5.000000\n",
      "Likes                              382.500000\n",
      "Subscribers lost                    13.000000\n",
      "Subscribers gained                  56.500000\n",
      "RPM(USD)                             4.370000\n",
      "CPM(USD)                            10.573000\n",
      "Average % viewed                    41.175000\n",
      "Views                             7417.000000\n",
      "Watch time (hours)                 279.985100\n",
      "Subscribers                         38.500000\n",
      "Your estimated revenue (USD)        24.799500\n",
      "Impressions                     155102.500000\n",
      "Impressions ctr(%)                   2.430000\n",
      "Avg_duration_sec                   166.000000\n",
      "Engagement_ratio                     0.060285\n",
      "Views / sub gained                 140.146406\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelida\\AppData\\Local\\Temp\\ipykernel_9768\\2869075216.py:2: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  median_agg = df_agg_diff[df_agg_diff['Video publish time'] >= metric_data_12mo].median()\n",
      "C:\\Users\\kelida\\AppData\\Local\\Temp\\ipykernel_9768\\2869075216.py:2: FutureWarning: The default value of numeric_only in DataFrame.median is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  median_agg = df_agg_diff[df_agg_diff['Video publish time'] >= metric_data_12mo].median()\n"
     ]
    }
   ],
   "source": [
    "metric_data_12mo = df_agg_diff['Video publish time'].max() - pd.DateOffset(months=12)\n",
    "median_agg = df_agg_diff[df_agg_diff['Video publish time'] >= metric_data_12mo].median()\n",
    "print(median_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ae811a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# alternative to the above cell:\n",
    "numeric_cols = np.array((df_agg_diff.dtypes == 'float64') | (df_agg_diff.dtypes == 'int64'))\n",
    "print(numeric_cols)\n",
    "df_agg_diff.iloc[:, numeric_cols] = (df_agg_diff.iloc[:,numeric_cols] - median_agg).div(median_agg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bad3fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelida\\AppData\\Local\\Temp\\ipykernel_9768\\1649207867.py:10: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  metric_median6mo = df_agg_metrics[df_agg_metrics['Video publish time'] >= metric_data_6mo].median()\n",
      "C:\\Users\\kelida\\AppData\\Local\\Temp\\ipykernel_9768\\1649207867.py:11: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  metric_median12mo = df_agg_metrics[df_agg_metrics['Video publish time'] >= metric_data_12mo].median()\n"
     ]
    }
   ],
   "source": [
    "if add_sidebar == 'Aggregate Metrics':\n",
    "     #bit of data enginering\n",
    "     # metrics that are relevant in descending order\n",
    "     df_agg_metrics = df_agg[['Video publish time','Views','Likes','Subscribers','Shares','Comments added','RPM(USD)','Average % viewed',\n",
    "                             'Avg_duration_sec', 'Engagement_ratio','Views / sub gained']]\n",
    "     # figure the most 06/12 month date range \n",
    "     metric_data_6mo = df_agg_metrics['Video publish time'].max() - pd.DateOffset(months=6)\n",
    "     metric_data_12mo = df_agg_metrics['Video publish time'].max() - pd.DateOffset(months=12)\n",
    "     #take the aggregate of all of the medians\n",
    "     metric_median6mo = df_agg_metrics[df_agg_metrics['Video publish time'] >= metric_data_6mo].median()\n",
    "     metric_median12mo = df_agg_metrics[df_agg_metrics['Video publish time'] >= metric_data_12mo].median()\n",
    "\n",
    "     # instead of metrics we could use columns\n",
    "     # create 5 columnsin the stramlit dashboard\n",
    "     col1, col2, col3, col4, col5 = st.columns(5)\n",
    "     columns = [col1, col2, col3, col4, col5]\n",
    "\n",
    "     # loop through all the diff columns\n",
    "     count = 0\n",
    "     for i in metric_median6mo.index:\n",
    "          with columns[count]:\n",
    "               delta = (metric_median6mo[i] - metric_median12mo[i])/metric_median12mo[i]\n",
    "               st.metric(label= i, value = round(metric_median6mo[i],1), delta = '{:.2%}'.format(delta))\n",
    "               count += 1\n",
    "               if count >= 5:\n",
    "                    count = 0\n",
    "\n",
    "     # add df to the dashboard\n",
    "     #get date information / trim to relevant data \n",
    "     df_agg_diff['Publish_date'] = df_agg_diff['Video publish time'].apply(lambda x: x.date())\n",
    "     df_agg_diff_final = df_agg_diff.loc[:,['Video title','Publish_date','Views','Likes','Subscribers','Shares','Comments added','RPM(USD)','Average % viewed',\n",
    "                             'Avg_duration_sec', 'Engagement_ratio','Views / sub gained']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd9ce01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the time data merge dailly with publish datas to get delta\n",
    "df_time_diff = pd.merge(df_time, df_agg.loc[:,['Video', 'Video publish time']], left_on = 'External Video ID', right_on='Video')\n",
    "df_time_diff['days_published'] = (df_time_diff['Date'] - df_time_diff['Video publish time']).dt.days\n",
    "\n",
    "#get last 12 mo of data rather than all data\n",
    "date_12mo = df_agg['Video publish time'].max() - pd.DateOffset(months=12)\n",
    "df_time_diff_yr = df_time_diff[df_time_diff['Video publish time'] >= date_12mo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3507f7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days_published</th>\n",
       "      <th>median_views</th>\n",
       "      <th>80pct_views</th>\n",
       "      <th>20pct_views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1651.5</td>\n",
       "      <td>4364.0</td>\n",
       "      <td>1106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2407.5</td>\n",
       "      <td>6655.0</td>\n",
       "      <td>1471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2791.5</td>\n",
       "      <td>7841.0</td>\n",
       "      <td>1680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3089.5</td>\n",
       "      <td>8605.0</td>\n",
       "      <td>1827.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>3281.5</td>\n",
       "      <td>9252.6</td>\n",
       "      <td>1949.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>3461.5</td>\n",
       "      <td>9789.0</td>\n",
       "      <td>2051.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>3619.5</td>\n",
       "      <td>10180.4</td>\n",
       "      <td>2131.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>3727.0</td>\n",
       "      <td>10488.6</td>\n",
       "      <td>2189.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>3831.5</td>\n",
       "      <td>10720.8</td>\n",
       "      <td>2234.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9</td>\n",
       "      <td>3925.5</td>\n",
       "      <td>10916.8</td>\n",
       "      <td>2275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>3996.5</td>\n",
       "      <td>11152.2</td>\n",
       "      <td>2319.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11</td>\n",
       "      <td>4065.5</td>\n",
       "      <td>11343.0</td>\n",
       "      <td>2357.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12</td>\n",
       "      <td>4134.5</td>\n",
       "      <td>11515.6</td>\n",
       "      <td>2397.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13</td>\n",
       "      <td>4197.5</td>\n",
       "      <td>11659.8</td>\n",
       "      <td>2432.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14</td>\n",
       "      <td>4245.5</td>\n",
       "      <td>11781.4</td>\n",
       "      <td>2462.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15</td>\n",
       "      <td>4290.5</td>\n",
       "      <td>11873.0</td>\n",
       "      <td>2490.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16</td>\n",
       "      <td>4335.5</td>\n",
       "      <td>11977.4</td>\n",
       "      <td>2514.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17</td>\n",
       "      <td>4383.5</td>\n",
       "      <td>12089.8</td>\n",
       "      <td>2540.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18</td>\n",
       "      <td>4429.0</td>\n",
       "      <td>12189.4</td>\n",
       "      <td>2564.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19</td>\n",
       "      <td>4466.0</td>\n",
       "      <td>12318.0</td>\n",
       "      <td>2586.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20</td>\n",
       "      <td>4506.0</td>\n",
       "      <td>12425.6</td>\n",
       "      <td>2610.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21</td>\n",
       "      <td>4542.0</td>\n",
       "      <td>12521.8</td>\n",
       "      <td>2630.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>22</td>\n",
       "      <td>4575.0</td>\n",
       "      <td>12596.4</td>\n",
       "      <td>2648.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23</td>\n",
       "      <td>4611.0</td>\n",
       "      <td>12674.2</td>\n",
       "      <td>2666.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>24</td>\n",
       "      <td>4646.0</td>\n",
       "      <td>12748.2</td>\n",
       "      <td>2689.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>25</td>\n",
       "      <td>4679.0</td>\n",
       "      <td>12831.2</td>\n",
       "      <td>2709.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>26</td>\n",
       "      <td>4711.0</td>\n",
       "      <td>12932.2</td>\n",
       "      <td>2729.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>27</td>\n",
       "      <td>4740.0</td>\n",
       "      <td>13014.2</td>\n",
       "      <td>2749.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>28</td>\n",
       "      <td>4772.0</td>\n",
       "      <td>13081.2</td>\n",
       "      <td>2763.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>29</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>13144.2</td>\n",
       "      <td>2779.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>30</td>\n",
       "      <td>4823.0</td>\n",
       "      <td>13195.2</td>\n",
       "      <td>2794.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    days_published  median_views  80pct_views  20pct_views\n",
       "4                0        1651.5       4364.0       1106.0\n",
       "5                1        2407.5       6655.0       1471.0\n",
       "6                2        2791.5       7841.0       1680.0\n",
       "7                3        3089.5       8605.0       1827.0\n",
       "8                4        3281.5       9252.6       1949.4\n",
       "9                5        3461.5       9789.0       2051.2\n",
       "10               6        3619.5      10180.4       2131.8\n",
       "11               7        3727.0      10488.6       2189.4\n",
       "12               8        3831.5      10720.8       2234.4\n",
       "13               9        3925.5      10916.8       2275.0\n",
       "14              10        3996.5      11152.2       2319.6\n",
       "15              11        4065.5      11343.0       2357.8\n",
       "16              12        4134.5      11515.6       2397.8\n",
       "17              13        4197.5      11659.8       2432.8\n",
       "18              14        4245.5      11781.4       2462.2\n",
       "19              15        4290.5      11873.0       2490.2\n",
       "20              16        4335.5      11977.4       2514.6\n",
       "21              17        4383.5      12089.8       2540.6\n",
       "22              18        4429.0      12189.4       2564.6\n",
       "23              19        4466.0      12318.0       2586.8\n",
       "24              20        4506.0      12425.6       2610.8\n",
       "25              21        4542.0      12521.8       2630.0\n",
       "26              22        4575.0      12596.4       2648.0\n",
       "27              23        4611.0      12674.2       2666.8\n",
       "28              24        4646.0      12748.2       2689.8\n",
       "29              25        4679.0      12831.2       2709.8\n",
       "30              26        4711.0      12932.2       2729.8\n",
       "31              27        4740.0      13014.2       2749.8\n",
       "32              28        4772.0      13081.2       2763.8\n",
       "33              29        4800.0      13144.2       2779.8\n",
       "34              30        4823.0      13195.2       2794.8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get daily view data(first 30), median & percentiles the below code could be done by groupby \n",
    "# but we use pivot tables\n",
    "views_days = pd.pivot_table(df_time_diff_yr, index='days_published', values='Views', aggfunc=[np.mean,np.median,lambda x: np.percentile(x, 80),lambda x: np.percentile(x, 20)]).reset_index()\n",
    "views_days.columns = ['days_published','mean_views','median_views','80pct_views','20pct_views']\n",
    "#take the days between 0 and 30\n",
    "views_days = views_days[views_days['days_published'].between(0,30)]\n",
    "views_cumulative = views_days.loc[:,['days_published','median_views','80pct_views','20pct_views']]\n",
    "views_cumulative.loc[:,['median_views','80pct_views','20pct_views']] = views_cumulative.loc[:,['median_views','80pct_views','20pct_views']].cumsum()\n",
    "\n",
    "## build dashboard\n",
    "views_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28bf833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Views', 'Likes', 'Subscribers', 'Shares', 'Comments added', 'RPM(USD)', 'Average % viewed', 'Avg_duration_sec', 'Engagement_ratio', 'Views / sub gained']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelida\\AppData\\Local\\Temp\\ipykernel_9768\\3612223704.py:1: FutureWarning: The default value of numeric_only in DataFrame.median is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  df_agg_numeric_lst = df_agg_diff_final.median().index.tolist()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Views': <function str.format>,\n",
       " 'Likes': <function str.format>,\n",
       " 'Subscribers': <function str.format>,\n",
       " 'Shares': <function str.format>,\n",
       " 'Comments added': <function str.format>,\n",
       " 'RPM(USD)': <function str.format>,\n",
       " 'Average % viewed': <function str.format>,\n",
       " 'Avg_duration_sec': <function str.format>,\n",
       " 'Engagement_ratio': <function str.format>,\n",
       " 'Views / sub gained': <function str.format>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg_numeric_lst = df_agg_diff_final.median().index.tolist()\n",
    "print(df_agg_numeric_lst)\n",
    "df_to_pct = {}\n",
    "for i in df_agg_numeric_lst:\n",
    "    df_to_pct[i] = '{:.1%}'.format\n",
    "df_to_pct    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e2c83ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Should You Be Excited About Web 3? (As a Data Scientist)',\n",
       " 'Should  @Luke Barousse Take This Data Analyst Job? (Funny) #Shorts',\n",
       " 'The Only Data Science Explanation You Need',\n",
       " 'We Need to Talk About The LinkedIn Machine Learning Assessment.',\n",
       " 'How I Would Learn Data Science in 2022 (If I Had to Start Over)',\n",
       " 'What the Heck is WSL 2? (My New Favorite Tool)',\n",
       " 'How Statistics Saved the US SERIOUS $$$$ During WW2 #Shorts',\n",
       " 'How Zillow Lost $500 MILLION With Machine Learning',\n",
       " 'Why Is Data Engineering So HOT Right Now?',\n",
       " 'Is Data Science Dying?',\n",
       " '7 Incredible Books That Transformed My Health and My Life',\n",
       " 'Why EVERYONE Should Start a Podcast (Including YOU)',\n",
       " 'Building a Burrito Dashboard - Data Science Project from Scratch with atoti',\n",
       " 'ML Ops: What is it REALLY?',\n",
       " 'Business Skills for Data Science: What are they REALLY?',\n",
       " '#66DaysOfData Round 3 Live Event! (feat. @StatQuest with Josh Starmer)',\n",
       " 'The TRUTH About My First Data Science Project',\n",
       " \"Is Data Visualization Important for Data Science? (A Data Scientist's Perspective)\",\n",
       " 'Project Presentation - Expectations vs. Reality (funny) #shorts',\n",
       " 'Data Science Project - Expectations vs Reality (funny) #shorts',\n",
       " 'How I Learned to Learn.',\n",
       " 'Is Spotify Shuffle Really Random? #Shorts',\n",
       " 'I Eat a Papaya Live on Stream (Plus Q&A for 150K Subs!)',\n",
       " 'Why are APIs Important for Data Science?',\n",
       " 'Is Your Phone REALLY Listening to You?',\n",
       " 'Kaggle vs Github - Which is Best for Your Data Science Portfolio?',\n",
       " '#66DaysOfData - 3 Reasons to Start!',\n",
       " '#66DaysOfData - What is it? #shorts',\n",
       " 'A Quick Data Science Project Tip! #SHORTS',\n",
       " 'Discouraged with Data Science? - Watch THIS video.',\n",
       " 'How Data Science Projects Pay Off',\n",
       " 'How I Use Data to Optimize My Life | What I Collect & How I Analyze It',\n",
       " 'What is a lambda function (python)? #shorts',\n",
       " 'Why Kaggle Should Be Your Favorite Data Science Resource #shorts',\n",
       " 'What is Pandas? (Data & Data Science) #shorts',\n",
       " 'The 7 Biggest Data Science  Beginner Mistakes',\n",
       " 'Data Scientist Reacts: REAL Data Science Job Application Data',\n",
       " \"Why You Probably Won't Become a Data Scientist\",\n",
       " 'How to Go From Data Analyst to Data Scientist',\n",
       " 'Why I Have 2 Offices for Data Science & Content Creation',\n",
       " 'MARCH MADNESS - Will My Machine Learning Model Beat Your Bracket?',\n",
       " 'My Regrets as a Data Science Student',\n",
       " 'My Daily Battle With Time - Will I Win? [Vlog]',\n",
       " 'What is the Future of my Comment Leaderboard Project?',\n",
       " 'How to Build a Website  - Building my ULTIMATE Portfolio Website',\n",
       " 'Why I Quit Data Science',\n",
       " 'ðŸŒ¶ Hot Topics in Tech: Data Science Explained #SHORTS',\n",
       " 'How I Balance Data Science and Content Creation (7 Secrets)',\n",
       " 'Kaggle Project From Scratch - Part 3 (Advanced Graphs & Gender Imbalance Analysis)',\n",
       " 'Kaggle Project From Scratch - Part 2 (Exploratory Data Analysis)',\n",
       " 'Kaggle Project From Scratch - Part 1 (Data Science Profession Survey)',\n",
       " 'What is the #66DaysOfData?',\n",
       " 'How I Would Learn Data Science in 2021 (What Has Changed?)',\n",
       " 'Unboxing the Ultimate Z by HP Data Science Package (FIRST EVER HP Workstation w/ Data Science Stack)',\n",
       " 'I Built the FIRST EVER YouTube Subscriber LEADERBOARD',\n",
       " '7 Things to Look For in a Masters For Data Science (feat. @Tina Huang)',\n",
       " '6 Lessons from #66DaysOfData',\n",
       " '100K Channel Update + AMA Stream!',\n",
       " 'Find a Data Science Project With These 3 Techniques',\n",
       " 'How I Chose My Masters Degree for Breaking into Data Science',\n",
       " 'git for Data Science Made Simple... (Hopefully)',\n",
       " 'The PODCAST you might have asked for?',\n",
       " 'Dealing with Doubt in Data Science (My Impostor Syndrome Story)',\n",
       " 'Advice from a Data Analytics CEO (@How to Get an Analytics Job) - KNN EP. 17',\n",
       " 'Data Science Resume Round-Up With @Tina Huang  | Episode 3',\n",
       " '5 Unusual Data Science Projects that Will Land You a Job',\n",
       " 'Sh*t Data Scientists Say (Parody)',\n",
       " '5 Proven Strategies to Break into a Data Science Job',\n",
       " 'Why Data-Viz is so Darn Important (@Story by Data | Kate Strachnyi) - KNN EP. 16',\n",
       " 'Data Science Resume Round-Up With @Tina Huang  - Episode 1',\n",
       " '5 Essential Data Science Projects for Your Portfolio',\n",
       " 'His Startup Will Land You a Data Science Job (Jeremie Harris) - KNN EP. 15',\n",
       " 'Reviewing Your Data Science Projects - Episode 21(The Cleanest Portfolio)',\n",
       " 'Hedge Funds, Startups, and Data Science Oh my! (@DataLeap) - KNN EP. 14',\n",
       " 'Land a Data Science Job in a Different Country (Vijay Pravin Maharajan) - KNN EP. 13',\n",
       " 'Reviewing Your Data Science Projects - Episode 20 (Bootcamp Capstone)',\n",
       " 'The 5 Pillars of Success I Live By',\n",
       " 'Fast Cars to Faster Data (Alex Castrounis) - KNN EP. 12',\n",
       " 'Reviewing Your Data Science Projects - Episode 19 (One Big Improvement)',\n",
       " \"How I Learn Data Science Through Studying Other People's Code | #66DaysOfData\",\n",
       " 'How She Dominated the FAANG Data Science Interview  (@Tina Huang ) - KNN EP. 11',\n",
       " 'Reviewing Your Data Science Projects - Episode 18 (Job-Worthy GitHub)',\n",
       " \"Why I'm Starting Data Science Over Again.\",\n",
       " 'Inside the Mind of the Ultimate Kaggle Grandmaster (@Abhishek Thakur ) - KNN EP. 10',\n",
       " 'Reviewing Your Data Science Projects - Episode 17 (Best Portfolio Website Yet?)',\n",
       " \"Don't Buy My Course..\",\n",
       " 'How a Subscriber Landed a Data Analyst Job in Less Than a Year (Ray Ojel) - KNN EP. 09',\n",
       " 'Reviewing Your Projects - Episode 16 (Project Review for Beginners)',\n",
       " 'The YouTube Algorithm EXPLAINED! (Tips from a Data Scientist)',\n",
       " 'Sports Analytics & Streaming Data Science on Twitch (Nick Wan) - KNN EP. 08',\n",
       " 'Reviewing Your Data Science Projects - Episode 15 (Quant Finance)',\n",
       " 'The Plagiarism Problem in Data Science',\n",
       " 'Interview with the Director of AI Research @ NVIDIA (Anima Anandkumar) - KNN EP. 07',\n",
       " 'Reviewing Your Data Science Projects - Episode 14 [Deep Learning Focus]',\n",
       " 'My First Data Science Contracting Side-Gig (How I Did It)',\n",
       " 'Do You Have a Data Science Mentor? (@Danny Ma) - KNN EP. 06',\n",
       " 'Reviewing Your Data Science Projects - Episode 13 (BONUS LinkedIn Review)',\n",
       " 'Building a Deep Learning BEAST (NVIDIA TITAN RTX + RYZEN 3900X)',\n",
       " 'Is it Important to Share Your Data Science Work? (Ft. Eric Weber)',\n",
       " 'Reviewing Your Data Science Resumes - Episode 12 (3 Different Resumes!)',\n",
       " 'Beginner Kaggle Data Science Project Walk-Through (Titanic)',\n",
       " 'Uber Driver to Machine Learning Engineer in 9 Months! (@Daniel Bourke) - KNN EP. 05',\n",
       " 'Reviewing Your Data Science Projects - Episode 11(GITHUB CLEANING)',\n",
       " 'The Best Computer for Data Science Beginners',\n",
       " 'Should You Major in Data Science? (Jaemin Lee) - KNN EP. 04',\n",
       " 'Critiquing MY OWN Data Science Resume',\n",
       " 'Where to Start Learning Data Science',\n",
       " 'Data Science Productivity, Motivation, and Organization (ft. Data Professor & Codebasics)',\n",
       " 'Reviewing Your Data Science Projects - Episode 10 (Leveraging Your Data)',\n",
       " 'How to Get a Data Science Job at FAANG (@Data Science Jay) - KNN EP. 03',\n",
       " 'Reviewing Your Data Science Projects - Episode 9 (Professional Violinist)',\n",
       " 'Ken Jee Q & A Live Stream (50,000 Sub Special!)',\n",
       " 'Reviewing Your Data Science Projects - Episode 8 (College Student Help)',\n",
       " \"Why You're Struggling to Learn Data Science\",\n",
       " 'Reviewing Your Data Science Projects - Episode 7 (Incredible Portfolio Website)',\n",
       " 'The State of Data Science with Krish Naik & The Data Professor [Panel Discussion]',\n",
       " 'Reviewing Your Data Science Projects - Episode 6 (Only 3 months of coding experience)',\n",
       " 'How to Build a Data Science Portfolio Website with Hugo & Github Pages [feat. Data Professor]',\n",
       " 'Reviewing Your Data Science Projects - Episode 5 (Very Detailed Project)',\n",
       " 'Different Data Science Roles Explained (by a Data Scientist)',\n",
       " 'Reviewing Your Data Science Projects - Episode 4 (Resume & Github)',\n",
       " 'How to Make A Data Science Portfolio Website with Github Pages',\n",
       " '10000 Subscriber and 100th Video Special (Data Science)',\n",
       " 'Reviewing Your Data Science Projects - Episode 3 (Student Portfolio)',\n",
       " 'How I Would Learn Data Science (If I Had to Start Over)',\n",
       " 'Reviewing Your Data Science Projects - Episode 2 (Resume and Portfolio)',\n",
       " 'What You Need to Know for a Data Science Internship',\n",
       " 'Reviewing Your Data Science Projects - Episode 1 (Exploratory Analysis)',\n",
       " \"What It's Like to be a Socially Distanced Data Scientist (A Day in the Life)\",\n",
       " 'Data Science in Sports - Talk for Northwestern (Kellogg) MBA Students',\n",
       " '5 Tips for Crushing the Work From Home Life',\n",
       " 'Data Science Project from Scratch - Part 7 (Documenting Your Work)',\n",
       " 'Data Science Project from Scratch -  Part 6 (Putting the Model into Production)',\n",
       " 'Data Science Project from Scratch  - Part 5 (Model Building)',\n",
       " 'Data Science Project from Scratch - Part 4 (Exploratory Data Analysis)',\n",
       " 'Data Science Project from Scratch - Part 3 (Data Cleaning)',\n",
       " 'Data Science Project from Scratch - Part 2 (Data Collection)',\n",
       " 'Data Science Project from Scratch - Part 1 (Project Planning)',\n",
       " 'When Data Science Goes Wrong',\n",
       " 'How to ULTRALEARN Data Science',\n",
       " 'Why Right NOW is a Great Time to Learn Data Science',\n",
       " 'Data Science Project Example Start to Finish (Deep Learning Image Classifier)',\n",
       " \"The Secret Data Scientists Don't Want You to Know\",\n",
       " 'The 5 Stages of Learning Data Science',\n",
       " 'Can You Learn Data Science Without a Computer?',\n",
       " 'The Best Free Data Science Courses Nobody is Talking About',\n",
       " 'The Data Science Projects that Got Me a Job',\n",
       " 'How to Integrate Data Science into Your Business',\n",
       " 'The Problem with Data Science',\n",
       " 'How Much Did Cheating Help the Astros Win? (What the Numbers Say)',\n",
       " 'How to Set Up Your Data Science Environment (Anaconda Beginner)',\n",
       " '3 Proven Data Science Projects for Beginners (Kaggle)',\n",
       " 'Data Science Advice for College Students',\n",
       " 'Avoid These Data Science Resume Mistakes!',\n",
       " 'Data Science Explained with ... Cooking?',\n",
       " 'The 4 Types of Sports Analytics Projects',\n",
       " 'Data Science in Golf: PGA Merchandise Show 2020',\n",
       " 'How I Learned Data Science',\n",
       " 'What is Sports Analytics Really?',\n",
       " 'Data Science Certificate vs Bootcamp vs Masters Degree',\n",
       " 'How To Learn Programming for Data Science [3 Steps]',\n",
       " 'Data Science Fundamentals: SQL Queries',\n",
       " 'Questions You Should Ask Your Data Science Interviewers',\n",
       " 'The 9 Books That Changed My Perspective in 2019',\n",
       " 'The Data Science Interview: What to Expect',\n",
       " '5 Data Science Resolutions for 2020',\n",
       " 'How to Scrape NBA Data Using the nba_api Python Module',\n",
       " 'Collision Course: Sports Betting + Data Science',\n",
       " '3 Reasons You Should NOT Become a Data Scientist',\n",
       " '5 Sports Analytics Books to Get You Started',\n",
       " 'Math Needed for Mastering Data Science',\n",
       " 'The 5 Stages of Data Science Adoption',\n",
       " 'How YOU Can Land a Sports Analytics Job',\n",
       " 'Data Science Fundamentals: Linear Regression',\n",
       " 'Golf: Would You Rather Be the LONGEST or STRAIGHTEST Driver on the PGA Tour?',\n",
       " 'Sports Analytics 101: The Pythagorean Theorem of Sports',\n",
       " '9 Ways You Can Make Extra Income as a Data Scientist',\n",
       " 'Applying Data Science To My YouTube Data: My Surprising Findings',\n",
       " 'Why Selling Is An Important Data Science Skill',\n",
       " 'By The Numbers: Where Should The NBA Put a 4 Point Line?',\n",
       " 'Why is Balance Important in Data Science?',\n",
       " 'How to Stay Productive & Motivated When Learning Data Science',\n",
       " 'How Far Should the NBA 3-Point Line Actually Be?',\n",
       " '6 Habits of Successful Data Scientists',\n",
       " 'How To Build A Word Cloud From Scraped Data (Python)',\n",
       " \"Thank You For The Support | What's Next | Ken Jee | Data Science\",\n",
       " 'Is Data Science Right For You?',\n",
       " 'Data Science Fundamentals: Data Cleaning in Python',\n",
       " 'How To Get Data Science Experience (Without a Job)',\n",
       " 'Take Your Data Science Projects From Good to Great',\n",
       " 'The Projects You Should Do To Get A Data Science Job',\n",
       " 'What Does a Data Scientist Actually Do?',\n",
       " 'Data Science Fundamentals: Data Manipulation in Python (Pandas)',\n",
       " 'Data Science Fundamentals: Data Exploration in Python (Pandas)',\n",
       " 'Data Science: Pros and Cons',\n",
       " 'How I Got My First Data Science Internship (And How You Can Land One)',\n",
       " 'My Top 5 Data Science Internship Tips',\n",
       " 'Golf STATS: Strokes Gained Explained',\n",
       " 'Most Data Science Hopefuls Overlook This Important Skill',\n",
       " 'What I Learned From My Three Degrees',\n",
       " 'I Wish I Had Known THIS Before Starting in Data Science',\n",
       " 'NASA Physicist Turned Data Scientist (Tim Bowling) - KNN EP. 02',\n",
       " 'Should You Learn R for Data Science?',\n",
       " 'Scrape Twitter Data in Python with Twitterscraper Module',\n",
       " 'Work From Home Data Scientist: Day in the Life',\n",
       " 'Where to Look for Data Science Jobs',\n",
       " 'Data Science: Startup vs. Large Corporation',\n",
       " \"Data Science, Machine Learning, and AI: What's the Difference?\",\n",
       " 'Was Captain Marvel Bad? A Sentiment Analysis of Twitter Data',\n",
       " \"Why You DON'T Want to be a WFH Data Scientist\",\n",
       " 'Welcome To My Channel | Ken Jee | Data Science',\n",
       " 'Where YOU Should Start With Data Science Projects',\n",
       " 'Watch This Before Applying to Data Science Jobs',\n",
       " 'My Top 5 Data Science Resources for 2019',\n",
       " \"IT'S NOT TOO LATE TO LEARN CODE!\",\n",
       " 'The Best Way to Predict NBA Minutes Played',\n",
       " 'Demystifying Data Science Roles',\n",
       " 'How to Simulate NBA Games in Python',\n",
       " 'Should You Get A Masters in Data Science?',\n",
       " 'How I Became A Data Scientist From a Business Background',\n",
       " 'Predicting Season Long NBA Wins Using Multiple Linear Regression',\n",
       " 'Predicting Crypto-Currency Price Using RNN lSTM & GRU',\n",
       " 'ProjectDemoCSC478_UFCFightData')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos = tuple(df_agg['Video title'])\n",
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c09d19c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-07-17 00:00:00')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_data_6mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79c12535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Views                 6062.000000\n",
       "Likes                  305.000000\n",
       "Subscribers             22.000000\n",
       "Shares                  41.000000\n",
       "Comments added          36.000000\n",
       "RPM(USD)                 4.395000\n",
       "Average % viewed        41.225000\n",
       "Avg_duration_sec       175.000000\n",
       "Engagement_ratio         0.067113\n",
       "Views / sub gained     146.869727\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_median6mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d84a0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Video', 'Video title', 'Video publish time', 'Comments added',\n",
       "       'Shares', 'Dislikes', 'Likes', 'Subscribers lost', 'Subscribers gained',\n",
       "       'RPM(USD)', 'CPM(USD)', 'Average % viewed', 'Average view duration',\n",
       "       'Views', 'Watch time (hours)', 'Subscribers',\n",
       "       'Your estimated revenue (USD)', 'Impressions', 'Impressions ctr(%)',\n",
       "       'Avg_duration_sec', 'Engagement_ratio', 'Views / sub gained'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8026119b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video title</th>\n",
       "      <th>Publish_date</th>\n",
       "      <th>Views</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Subscribers</th>\n",
       "      <th>Shares</th>\n",
       "      <th>Comments added</th>\n",
       "      <th>RPM(USD)</th>\n",
       "      <th>Average % viewed</th>\n",
       "      <th>Avg_duration_sec</th>\n",
       "      <th>Engagement_ratio</th>\n",
       "      <th>Views / sub gained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Should You Be Excited About Web 3? (As a Data ...</td>\n",
       "      <td>2022-01-17</td>\n",
       "      <td>-0.409060</td>\n",
       "      <td>-0.301961</td>\n",
       "      <td>-0.896104</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>-0.149425</td>\n",
       "      <td>-0.072082</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>-0.048193</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.737469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Should  @Luke Barousse Take This Data Analyst ...</td>\n",
       "      <td>2022-01-14</td>\n",
       "      <td>-0.676284</td>\n",
       "      <td>-0.796078</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.952941</td>\n",
       "      <td>-0.724138</td>\n",
       "      <td>-0.569336</td>\n",
       "      <td>0.716576</td>\n",
       "      <td>-0.771084</td>\n",
       "      <td>-0.343672</td>\n",
       "      <td>16.132084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>The Only Data Science Explanation You Need</td>\n",
       "      <td>2022-01-10</td>\n",
       "      <td>0.385601</td>\n",
       "      <td>0.887582</td>\n",
       "      <td>1.805195</td>\n",
       "      <td>2.317647</td>\n",
       "      <td>0.425287</td>\n",
       "      <td>0.366362</td>\n",
       "      <td>-0.138069</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.501088</td>\n",
       "      <td>-0.460805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>We Need to Talk About The LinkedIn Machine Lea...</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>0.592018</td>\n",
       "      <td>0.547712</td>\n",
       "      <td>0.766234</td>\n",
       "      <td>-0.152941</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.217620</td>\n",
       "      <td>0.094839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009619</td>\n",
       "      <td>0.080189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>How I Would Learn Data Science in 2022 (If I H...</td>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>9.689362</td>\n",
       "      <td>10.537255</td>\n",
       "      <td>64.116883</td>\n",
       "      <td>17.047059</td>\n",
       "      <td>1.505747</td>\n",
       "      <td>0.564302</td>\n",
       "      <td>-0.106982</td>\n",
       "      <td>0.620482</td>\n",
       "      <td>0.117670</td>\n",
       "      <td>-0.778411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Should You Get A Masters in Data Science?</td>\n",
       "      <td>2018-11-14</td>\n",
       "      <td>1.492652</td>\n",
       "      <td>-0.278431</td>\n",
       "      <td>1.051948</td>\n",
       "      <td>-0.035294</td>\n",
       "      <td>0.287356</td>\n",
       "      <td>0.692906</td>\n",
       "      <td>0.098239</td>\n",
       "      <td>-0.084337</td>\n",
       "      <td>-0.656364</td>\n",
       "      <td>0.628632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>How I Became A Data Scientist From a Business ...</td>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>-0.256438</td>\n",
       "      <td>-0.560784</td>\n",
       "      <td>1.103896</td>\n",
       "      <td>-0.223529</td>\n",
       "      <td>-0.747126</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>-0.069095</td>\n",
       "      <td>0.427711</td>\n",
       "      <td>-0.350323</td>\n",
       "      <td>-0.514176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Predicting Season Long NBA Wins Using Multiple...</td>\n",
       "      <td>2018-07-10</td>\n",
       "      <td>-0.074693</td>\n",
       "      <td>-0.584314</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>-0.839080</td>\n",
       "      <td>-0.340275</td>\n",
       "      <td>-0.345962</td>\n",
       "      <td>-0.126506</td>\n",
       "      <td>-0.485180</td>\n",
       "      <td>0.440301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Predicting Crypto-Currency Price Using RNN lST...</td>\n",
       "      <td>2017-11-18</td>\n",
       "      <td>1.232439</td>\n",
       "      <td>-0.354248</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>1.682353</td>\n",
       "      <td>-0.356322</td>\n",
       "      <td>-0.696568</td>\n",
       "      <td>-0.172799</td>\n",
       "      <td>-0.367470</td>\n",
       "      <td>-0.592267</td>\n",
       "      <td>0.064395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>ProjectDemoCSC478_UFCFightData</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>-0.991910</td>\n",
       "      <td>-0.997386</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.952941</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.988558</td>\n",
       "      <td>-0.786521</td>\n",
       "      <td>-0.614458</td>\n",
       "      <td>-0.170608</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Video title Publish_date     Views  \\\n",
       "111  Should You Be Excited About Web 3? (As a Data ...   2022-01-17 -0.409060   \n",
       "187  Should  @Luke Barousse Take This Data Analyst ...   2022-01-14 -0.676284   \n",
       "64          The Only Data Science Explanation You Need   2022-01-10  0.385601   \n",
       "59   We Need to Talk About The LinkedIn Machine Lea...   2022-01-03  0.592018   \n",
       "32   How I Would Learn Data Science in 2022 (If I H...   2021-12-27  9.689362   \n",
       "..                                                 ...          ...       ...   \n",
       "75           Should You Get A Masters in Data Science?   2018-11-14  1.492652   \n",
       "190  How I Became A Data Scientist From a Business ...   2018-11-12 -0.256438   \n",
       "204  Predicting Season Long NBA Wins Using Multiple...   2018-07-10 -0.074693   \n",
       "138  Predicting Crypto-Currency Price Using RNN lST...   2017-11-18  1.232439   \n",
       "223                     ProjectDemoCSC478_UFCFightData   2017-06-06 -0.991910   \n",
       "\n",
       "         Likes  Subscribers     Shares  Comments added  RPM(USD)  \\\n",
       "111  -0.301961    -0.896104   0.011765       -0.149425 -0.072082   \n",
       "187  -0.796078    -1.000000  -0.952941       -0.724138 -0.569336   \n",
       "64    0.887582     1.805195   2.317647        0.425287  0.366362   \n",
       "59    0.547712     0.766234  -0.152941        0.494253  0.217620   \n",
       "32   10.537255    64.116883  17.047059        1.505747  0.564302   \n",
       "..         ...          ...        ...             ...       ...   \n",
       "75   -0.278431     1.051948  -0.035294        0.287356  0.692906   \n",
       "190  -0.560784     1.103896  -0.223529       -0.747126  0.011213   \n",
       "204  -0.584314    -0.142857   0.058824       -0.839080 -0.340275   \n",
       "138  -0.354248     1.857143   1.682353       -0.356322 -0.696568   \n",
       "223  -0.997386    -1.000000  -0.952941       -1.000000 -0.988558   \n",
       "\n",
       "     Average % viewed  Avg_duration_sec  Engagement_ratio  Views / sub gained  \n",
       "111          0.177899         -0.048193          0.343527            0.737469  \n",
       "187          0.716576         -0.771084         -0.343672           16.132084  \n",
       "64          -0.138069          0.686747          0.501088           -0.460805  \n",
       "59           0.094839          0.000000         -0.009619            0.080189  \n",
       "32          -0.106982          0.620482          0.117670           -0.778411  \n",
       "..                ...               ...               ...                 ...  \n",
       "75           0.098239         -0.084337         -0.656364            0.628632  \n",
       "190         -0.069095          0.427711         -0.350323           -0.514176  \n",
       "204         -0.345962         -0.126506         -0.485180            0.440301  \n",
       "138         -0.172799         -0.367470         -0.592267            0.064395  \n",
       "223         -0.786521         -0.614458         -0.170608                 inf  \n",
       "\n",
       "[223 rows x 12 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg_diff_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f6b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
